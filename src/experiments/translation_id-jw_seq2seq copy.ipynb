{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../\")))\n",
    "\n",
    "from data.preprocessing import preprocess_dataset  # ✅ Import ulang setelah menambahkan path\n",
    "from data.data_loader import get_data_loader\n",
    "from utils.training import init_model, train_fn, evaluate_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/gru_translation/bin/python\n",
      "Python 3.10.16\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.5\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['indonesia', 'jawa'],\n",
      "        num_rows: 800\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['indonesia', 'jawa'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['indonesia', 'jawa'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "dataset_path = os.path.abspath(\"../../data/indonesia_jawa_dataset\")\n",
    "\n",
    "dataset = load_from_disk(dataset_path)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indonesia': 'Mau bikin postingan yang isinya mengedukasi customers gojek.',\n",
       " 'jawa': 'Pengin nggawe postingan sing isine ngajari pelanggan Gojek.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 800/800 [00:00<00:00, 1351.63 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 337.28 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 461.43 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenisasi selesai dengan BERT tokenizer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 800/800 [00:00<00:00, 2612.52 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2449.37 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1340.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data siap digunakan dalam format PyTorch!\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "train_data, valid_data, test_data, en_vocab, id_vocab = preprocess_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil indeks padding dari vocabulary\n",
    "pad_index = en_vocab[\"<pad>\"]\n",
    "\n",
    "# Definisikan batch size\n",
    "batch_size = 128  # Sesuai kebutuhan\n",
    "\n",
    "# Buat DataLoader untuk train, valid, dan test\n",
    "train_loader = get_data_loader(train_data, batch_size=batch_size, pad_index=pad_index, shuffle=True)\n",
    "valid_loader = get_data_loader(valid_data, batch_size=batch_size, pad_index=pad_index, shuffle=False)\n",
    "test_loader = get_data_loader(test_data, batch_size=batch_size, pad_index=pad_index, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(2622, 512)\n",
       "    (rnn): GRU(512, 1024)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(3113, 512)\n",
       "    (rnn): GRU(1536, 1024)\n",
       "    (fc_out): Linear(in_features=2560, out_features=3113, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inisialisasi Model\n",
    "input_dim = len(id_vocab)\n",
    "output_dim = len(en_vocab)\n",
    "embedding_dim = 512\n",
    "hidden_dim = 1024\n",
    "dropout = 0.3\n",
    "clip = 1.0\n",
    "teacher_forcing_initial = 0.5\n",
    "teacher_forcing_final = 0.1\n",
    "epochs = 10\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model, optimizer, criterion = init_model(input_dim, output_dim, embedding_dim, hidden_dim, dropout, pad_index, device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "best_valid_loss = float(\"inf\")\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    teacher_forcing_ratio = teacher_forcing_initial - \\\n",
    "                            (teacher_forcing_initial - teacher_forcing_final) * \\\n",
    "                            (epoch / (epochs - 1))\n",
    "    \n",
    "    train_loss = train_fn(model, train_loader, optimizer, criterion, clip, teacher_forcing_ratio, device)\n",
    "    valid_loss = evaluate_fn(model, valid_loader, criterion, device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"checkpoints/gru_model.pt\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):.3f}\")\n",
    "    print(f\"Valid Loss: {valid_loss:.3f} | Valid PPL: {np.exp(valid_loss):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi Model\n",
    "model.load_state_dict(torch.load(\"gru_model.pt\"))\n",
    "test_loss = evaluate_fn(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terjemahan: . . . . . . . . . . . . . . . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "# sentence = \"Dia sangat pintar.\"\n",
    "translated = translate_sentence(sentence, model, tokenizer, en_vocab, id_vocab, \"<sos>\", \"<eos>\", device)\n",
    "print(\"Terjemahan:\", translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terjemahan: . .\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    en_vocab,\n",
    "    id_vocab,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=25,\n",
    "    max_repetition=3  # Batas maksimum pengulangan token tidak bermakna\n",
    "):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenisasi input\n",
    "    tokens = [sos_token] + tokenizer.tokenize(sentence.lower())[:1000] + [eos_token]\n",
    "    numericalized = [id_vocab[token] if token in id_vocab else id_vocab[\"<unk>\"] for token in tokens]\n",
    "    sentence_tensor = torch.LongTensor(numericalized).unsqueeze(1).to(device)\n",
    "\n",
    "    # Encoder\n",
    "    with torch.no_grad():\n",
    "        context = model.encoder(sentence_tensor)\n",
    "\n",
    "    # Decoder\n",
    "    trg_tokens = [en_vocab[sos_token]]\n",
    "    hidden = context\n",
    "    last_token = None\n",
    "    repeat_count = 0\n",
    "\n",
    "    for _ in range(max_output_length):\n",
    "        trg_tensor = torch.LongTensor([trg_tokens[-1]]).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor, hidden, context)\n",
    "        \n",
    "        # Ambil token dengan probabilitas tertinggi\n",
    "        top1 = output.argmax(1).item()\n",
    "        trg_tokens.append(top1)\n",
    "\n",
    "        # Cek pengulangan token tidak bermakna\n",
    "        current_token = en_vocab.lookup_token(top1)\n",
    "        if current_token in [\".\", \"<pad>\", \"<unk>\"]:  # Token yang dianggap tidak bermakna\n",
    "            if current_token == last_token:\n",
    "                repeat_count += 1\n",
    "            else:\n",
    "                repeat_count = 1\n",
    "            if repeat_count >= max_repetition:\n",
    "                break  # Hentikan jika pengulangan melebihi batas\n",
    "        else:\n",
    "            repeat_count = 0  # Reset jika token bermakna\n",
    "        \n",
    "        last_token = current_token\n",
    "\n",
    "        # Hentikan jika menemukan <eos>\n",
    "        if top1 == en_vocab[eos_token]:\n",
    "            break\n",
    "\n",
    "    # Konversi ke string\n",
    "    translated_tokens = en_vocab.lookup_tokens(trg_tokens[1:-1])  # Hapus <sos> & <eos>\n",
    "    translated_sentence = \" \".join(translated_tokens)\n",
    "    return translated_sentence\n",
    "\n",
    "# Contoh penggunaan\n",
    "sentence = \"aku akan makan\"\n",
    "translated = translate_sentence(sentence, model, tokenizer, en_vocab, id_vocab, \"<sos>\", \"<eos>\", device)\n",
    "print(\"Terjemahan:\", translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gru_translation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
