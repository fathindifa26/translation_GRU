{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../\")))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import tqdm\n",
    "import yaml \n",
    "from data.preprocessing import preprocess_dataset  \n",
    "from data.data_loader import get_data_loader\n",
    "from utils.training import init_model, train_fn, evaluate_fn\n",
    "from utils.inference import translate_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 **Load konfigurasi YAML**\n",
    "with open(os.path.abspath(\"../../configs/gru_seq2seq.yaml\"), \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text_1', 'text_2', 'text_1_lang', 'text_2_lang'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'text_1', 'text_2', 'text_1_lang', 'text_2_lang'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text_1', 'text_2', 'text_1_lang', 'text_2_lang'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(config[\"data\"][\"dataset_path\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '10',\n",
       " 'text_1': 'Wektu kuwi mara mrene pesen sega goreng karo kentang goreng, sega gorenge kabehane seneng, Kentang gorenge enak tenan, lan dhelokane apik. Keluarga ngomong yen kopine enak. Babagan paling apik nang kene yaiku panggonane jembar lan sek nang njaba iso ndhelok pemandangan dusun pring, nanging pelayanane ora cepet dadi aku kudu takon kaping pirang-pirang babagan pesenanku. Regane cukup larang, nanging amarga panganane enak kabeh dadi kabayar. Ora kudhu mikir ping pindho yen pengen mara panggonan iki maneh.',\n",
       " 'text_2': 'Waktu itu ke sini pesan nasi goreng dan kentang goreng, nasi gorengnya semua suka. Kentang gorengnya enak banget, dan presentasinya bagus. Keluarga bilang kopinya enak. Hal yang sangat baik di sini adalah tempatnya luas dan yang di luar bisa lihat pemandangan dusun bambu, tapi pelayanannya tidak cepat sehingga saya harus bertanya beberapa kali tentang pesanan saya. Harganya cukup mahal, namun karena makanannya enak semua jadi terbayarkan. Tidak perlu pikir dua kali jika mau ke tempat ini lagi.',\n",
       " 'text_1_lang': 'jav',\n",
       " 'text_2_lang': 'ind'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nikmati cicilan 0% hingga 12 bulan untuk pemesanan tiket pesawat air asia dengan kartu kredit bni!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]['text_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 800/800 [00:00<00:00, 1257.51 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1227.89 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 614.50 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenisasi selesai dengan BERT tokenizer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 800/800 [00:00<00:00, 2328.66 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2025.07 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2191.50 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data siap digunakan dalam format PyTorch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "train_data, valid_data, test_data, en_vocab, id_vocab = preprocess_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil indeks padding dari vocabulary\n",
    "pad_index = en_vocab[config[\"data\"][\"pad_token\"]]\n",
    "\n",
    "\n",
    "# Definisikan batch size\n",
    "batch_size = config[\"training\"][\"batch_size\"]\n",
    "\n",
    "# Buat DataLoader untuk train, valid, dan test\n",
    "train_loader = get_data_loader(train_data, batch_size=batch_size, pad_index=pad_index, shuffle=True)\n",
    "valid_loader = get_data_loader(valid_data, batch_size=batch_size, pad_index=pad_index, shuffle=False)\n",
    "test_loader = get_data_loader(test_data, batch_size=batch_size, pad_index=pad_index, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(3149, 256)\n",
       "    (rnn): GRU(256, 512)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(3051, 256)\n",
       "    (rnn): GRU(768, 512)\n",
       "    (fc_out): Linear(in_features=1280, out_features=3051, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masukin ke YAML \n",
    "config[\"model\"][\"input_dim\"] = len(id_vocab)\n",
    "config[\"model\"][\"output_dim\"] = len(en_vocab)\n",
    "\n",
    "# Inisialisasi Model\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model, optimizer, criterion = init_model(\n",
    "    config[\"model\"][\"input_dim\"],\n",
    "    config[\"model\"][\"output_dim\"],\n",
    "    config[\"model\"][\"embedding_dim\"],\n",
    "    config[\"model\"][\"hidden_dim\"],\n",
    "    config[\"model\"][\"dropout\"],\n",
    "    pad_index,\n",
    "    device\n",
    ")\n",
    "\n",
    "# 📌 **Training Parameters dari YAML**\n",
    "epochs = config[\"training\"][\"epochs\"]\n",
    "clip = config[\"training\"][\"clip\"]\n",
    "teacher_forcing_initial = config[\"training\"][\"teacher_forcing_initial\"]\n",
    "teacher_forcing_final = config[\"training\"][\"teacher_forcing_final\"]\n",
    "checkpoint_path = config[\"training\"][\"checkpoint_path\"]\n",
    "patience = config[\"training\"][\"patience\"]\n",
    "patience_counter = config[\"training\"][\"patience_counter\"]\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, optimizer, criterion, config, resume_training=False):\n",
    "    \"\"\"\n",
    "    Function to train a sequence-to-sequence model with GRU.\n",
    "    Supports resuming training from a saved checkpoint.\n",
    "    \"\"\"\n",
    "    epochs = config[\"training\"][\"epochs\"]\n",
    "    clip = config[\"training\"][\"clip\"]\n",
    "    patience = config[\"training\"][\"patience\"]\n",
    "    checkpoint_path = config[\"training\"][\"checkpoint_path\"]\n",
    "\n",
    "    teacher_forcing_initial = config[\"training\"][\"teacher_forcing_initial\"]\n",
    "    teacher_forcing_final = config[\"training\"][\"teacher_forcing_final\"]\n",
    "\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    start_epoch = 0\n",
    "    history = {\"train_loss\": [], \"valid_loss\": [], \"train_ppl\": [], \"valid_ppl\": []}\n",
    "\n",
    "    if resume_training and os.path.exists(checkpoint_path):\n",
    "        print(f\"🔄 Resuming training from checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        best_valid_loss = checkpoint[\"best_valid_loss\"]\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        history = checkpoint[\"history\"]\n",
    "\n",
    "    model.to(config['training'][\"device\"])\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(start_epoch, epochs), desc=\"Training Progress\"):\n",
    "        teacher_forcing_ratio = teacher_forcing_initial - \\\n",
    "                                (teacher_forcing_initial - teacher_forcing_final) * \\\n",
    "                                (epoch / (epochs - 1))\n",
    "\n",
    "        train_loss = train_fn(model, train_loader, optimizer, criterion, clip, teacher_forcing_ratio, config['training'][\"device\"])\n",
    "        valid_loss = evaluate_fn(model, valid_loader, criterion, config['training'][\"device\"])\n",
    "\n",
    "        train_ppl = np.exp(train_loss)\n",
    "        valid_ppl = np.exp(valid_loss)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"valid_loss\"].append(valid_loss)\n",
    "        history[\"train_ppl\"].append(train_ppl)\n",
    "        history[\"valid_ppl\"].append(valid_ppl)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            patience_counter = 0\n",
    "\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"best_valid_loss\": best_valid_loss,\n",
    "                \"history\": history\n",
    "            }, checkpoint_path)\n",
    "\n",
    "            print(f\"✅ Model saved at epoch {epoch + 1} with valid loss: {valid_loss:.3f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        print(f\"📌 Epoch {epoch+1} | Train Loss: {train_loss:.3f} | Train PPL: {train_ppl:.3f}\")\n",
    "        print(f\"📌 Valid Loss: {valid_loss:.3f} | Valid PPL: {valid_ppl:.3f}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"⏹️ Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    with open(\"training_history.json\", \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "    print(\"🏁 Training Finished!\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(model, train_loader, valid_loader, optimizer, criterion, config, resume_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"training_history.json\", \"r\") as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"valid_loss\"], label=\"Valid Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 6.502 | Test PPL: 666.733\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi Model\n",
    "model.load_state_dict(torch.load(\"../checkpoints/gru_model.pt\"))\n",
    "test_loss = evaluate_fn(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terjemahan: pang ##anan ##e . ##e . ##e . .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Contoh model (Pastikan model sudah di-load sebelumnya)\n",
    "sentence = \"koe mangan opo\"\n",
    "translated = translate_sentence(sentence, model, en_vocab, id_vocab, \"<sos>\", \"<eos>\", device)\n",
    "print(\"Terjemahan:\", translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gru_translation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
